\section{MODELING WITH GAUSSIAN PROCESSES}
\label{S:gp}

As discussed in the Introduction section, conventional building modeling methods based on first principles are time-consuming and cost-prohibitive.
Data-driven modeling approaches, based on machine learning techniques, have been shown to be fast, economical, and accurate alternatives.
This section presents a building modeling approach with Gaussian Process (GP), starting with a brief introduction to GP and its application in controls.
\subsection{Introduction to Gaussian Process}
\label{sec:gp:gp-intro}

\begin{definition}%[\cite{Rasmussen2006}]
A Gaussian Process is a collection of random variables, any finite number of which have a joint Gaussian distribution.
\end{definition}
%
Consider an unknown function \(f: \RR^n \mapsto \RR\) with noisy observations \(y = f(x) + \GaussianDist{0}{\sigma_n^2}\).
A GP of \(y\) is specified by its mean function \(\mu(x)\) and covariance function \(k(x,x')\),
\begin{align}
\label{E:gp:prior}
\mu(x; \theta) &= \EE [f(x)] \\
k(x,x'; \theta) &= \EE [(f(x)\!-\!\mu(x)) (f(x') \!-\! \mu(x'))] + \sigma_n^2 \delta(x,x') \nonumber
\end{align}
where \(\delta(x,x')\) is the Kronecker delta function.
These functions are parameterized by the hyperparameter vector \(\theta\).
The covariance function \(k(x,x')\) specifies how the outputs at \(x\) and \(x'\) are correlated.
In other words, a GP model of \(y\) specifies the relationship between the input variables, through the covariance function, rather than a fixed structural input--output relationship from $x$ to $y$.

Given the regression vectors \(X = [x_1, \dots, x_N]^T\) and their corresponding observed outputs \(Y = [y_1, \dots, y_N]^T\), the distribution of the output \(y_\star\) corresponding to a new input vector \(x_\star\) is a Gaussian distribution \(\GaussianDist{\bar{y}_\star}{\sigma_\star^2}\),
\begin{subequations}
\label{E:gp-regression}
\begin{align}
\bar{y}_\star &= g_{\mathrm{m}} (x_{\star}) \coloneqq \mu(x_\star) + K_\star K^{-1} (Y - \mu(X))\\
\sigma_\star^2 &= g_{\mathrm{v}} (x_{\star}) \coloneqq K_{\star \star} - K_\star K^{-1} K_\star^T \text,
\end{align}
\end{subequations}
where \(K_\star = [k(x_\star, x_1), \dots, k(x_\star, x_N)]\), \(K_{\star \star} = k(x_\star, x_\star)\), and $K$ is the covariance matrix with elements \(K_{ij} = k(x_i, x_j)\).
The hyperparameters $\theta$ can be learned by maximizing the likelihood: \(\argmax_\theta \Pr(Y \vert X, \theta)\).
%It is therefore highly flexible and can capture complex behavior with fewer parameters.
An example of GP prior and posterior is shown in Fig.~\ref{F:gp:prior:posterior}. %, where a constant mean function and a covariance function consisting of a squared exponential kernel and a rational quadratic kernel are used.


\begin{figure}[!tb]
  \centering
  \setlength\fwidth{\columnwidth}
  \setlength\hwidth{0.5\fwidth}
  \resizebox{\columnwidth}{!}{\input{images/gp-prior-post.tex}}
  \caption{Example of priors calculated using \eqref{E:gp:prior} and
    posteriors using \eqref{E:gp-regression} for predicting power
    consumption of a building for \(12\) hrs. Initially the mean is
    constant because \(\mu(x)\) is constant, and we observe a high
    variance. The posterior agrees with the actual power consumption
    with high confidence.}
  \label{F:gp:prior:posterior}
\end{figure}

GP models have several advantages over other machine learning models, that make them more suitable for identification of dynamical systems.
\begin{enumerate}
\item GPs provide predictive variances, which carry uncertainty information of the predictions. The full predictive distribution, which includes both the mean and variance, can be used in a meaningful way, \eg to estimate a 95\% confidence bound for the prediction.
\item GPs work well with small data sets due to its stochastic nature, which is generally useful for any modeling application.
\item GPs allow incorporating domain knowledge of the system %behavior
  into the model to improve its accuracy, by defining priors on the hyperparameters or using a particular structure of the covariance function.
\end{enumerate}

More details on GPs and their applications can be found in \cite{Rasmussen2006}.

\subsection{Gaussian Processes for Dynamical Systems}
\label{SS:intro-gp:control}

Consider a nonlinear dynamical system with control input \(u\), exogenous disturbance input \(w\), and output \(y\).
At a current time step $t$, time-delayed input and output signals are called \emph{autoregressive}, for example: $y_{t-1}, u_{t-1}, w_{t-1}$.
By feeding autoregressive inputs and outputs to a GP as regressors, we can model the dynamic behavior of the system \cite{Kocijan2016}.
Specifically, the regressor vector $x_{t}$ at time step $t$ would be
\begin{equation*}
x_{t}\!=\![y_{t-l}, \dots, y_{t-1}, u_{t-m}, \dots, u_t, w_{t-p}, \dots, w_{t-1}, w_t] \text.
\end{equation*}
where \(l\), \(m\), and \(p\) are respectively the lags for autoregressive outputs, control inputs, and disturbances.
Note that \(u_t\) and \(w_t\) are the current control and disturbance inputs.
The dynamical GP
\begin{math}
y_{t} = f(x_t)
\end{math}
can then be trained from data in the same way as any other GPs.

In a multistep simulation of a dynamical GP, the autoregressive outputs fed to the model beyond the first step are random variables, resulting in more and more complex output distributions as we go further.
Therefore, it involves uncertainty propagation through the model, which would complicate the computation of the model significantly.
\cite{nghiemetal16gp} showed that a simple simulation method called \emph{zero-variance method}, which replaces the autoregressive signals with their corresponding expected values, could achieve sufficient prediction accuracy while benefitting from computational simplicity.
In this paper, the zero-variance method was selected for predicting future outputs in optimization formulations.


\subsection{Modeling Building's Power Demand and Zone Temperatures with Gaussian Processes}
\label{sec:gp:bldg-modeling}

We use a U.S. Department of Energyâ€™s Commercial Reference Building (DoE CRB) simulated in EnergyPlus as the virtual test-bed building. It is a large 12-story office building consisting of 19 zones with a total area of 498,588 sq.ft. Under peak load conditions the office can consume up to 1.4 MW of power.
Our aim is to model the building's power demand and zone temperature behavior from data that can be measured directly from installed sensors such as thermostats, multimeters and weather forecast.
We learn two GP models: \(\mathcal{M}_1\) for predicting the power demand and \(\mathcal{M}_2\) for predicting the temperature of a particular zone in the building.
The following feature variables are used for training:
\begin{itemize}
\item \textit{Weather variables \(d^w\):} such as outside temperature and humidity, which are derived from historical weather data.
\item \textit{Proxy variables \(d^p\):} such as time of day and day of week, which indicate occupancy and periodic trends.
%\textit{Fixed schedules  \(d^s\):} kitchen cooling set point, corridor cooling set point - these set points follow predefined rules. 
\item \textit{Control variables \(u\):} such as cooling, supply air temperature and chilled water setpoints -- these variables will be optimized in MPC.
\item \textit{Output variable \(y\):} total power consumption for\(\mathcal{M}_1\) and zone temperature for \(\mathcal{M}_2\) -- this is the output of interest which we will predict using all the above features in the model.
\end{itemize}

Using these measurement data, we learn autoregressive GP models \(\mathcal{M}_1\) and \(\mathcal{M}_2\), and use the zero-variance method to predict the future output \(y_{t+\tau}\), where $t$ is the current time and \( \tau \ge 0\).
Specifically,
\begin{gather}
\label{eq:dpc:prediction}
y_{t+\tau} \sim \GaussianDist{\bar{y}_{t+\tau} = g_{\mathrm m}(x_{t+\tau})}{\sigma^2_{y, t+\tau} = g_{\mathrm v}(x_{t+\tau})}, \\
x_{t + \tau} = [\bar y_{t+ \tau-l}, \dots, \bar y_{t+ \tau-1}, u_{t+ \tau-m}, \dots, u_{t+ \tau}, \nonumber \\
\qquad\qquad\qquad\qquad  w_{t+ \tau-p}, \dots, w_{t+ \tau-1}, w_{t+ \tau}]\text, \nonumber
\end{gather}
in which \(w:=[d^w, d^p]\).
It is assumed that at time \(t\), \(w_{t+\tau}\) are available \(\forall \tau \) from forecasts or fixed rules as applicable.
For the GP, we use a constant mean function and the special covariance function proposed in our previous work \cite{nghiemetal16gp} to capture both the temporal pattern of the energy usage and the effect of non-temporal features, such as weather conditions and temperature setpoints, on the power demand.
We optimize the hyperparameters \(\theta\) % = [\mu, k, \sigma_{f_2}, \lambda_d, \sigma_{f_3}, \alpha, \lambda] \)
of the GP model using GPML \cite{Rasmussen2010}.

\begin{figure}
	\missingfigure[figwidth=\linewidth]{Power demand and Temperature Prediction}
	\caption{Power demand and Temperature Prediction}
	\label{F:prediction}
\end{figure}
The prediction with 95\% confidence bounds for a particular day are shown in Figure \ref{F:prediction}.
\todo[inline]{add results with figure}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
